# TODO:
#
# Config to possibly add to Flags:
#
# - WORKERS
# - TREE

- config: common-flags
  flags:
    gpu:
      description: Index of CUDA device to run operation on
      default: 0
    batch_size:
      description: Training batch size
      default: 7
    embedding_dim:
      description: Embedding dimension
      default: 1024
    captions_per_image:
      description: Number of captions per image
      default: 1
    words_num:
      description: Number of words per caption
      default: 244

- config: pretrain-train-flags
  # Shared across pretrain and train (i.e. training related
  # operations)
  flags:
    epochs:
      description: Number of epochs to run
      arg-name: max_epoch
      default: 201
    snapshot-interval:
      description: Number of epochs per snapshot
      default: 100
    train_split:
      description: Ratio of data used for training
      default: 1.0
    validation_split:
      description: Ration of data used for validation
      default: 1.0
    gamma1:
      description: Smooth gamma1
      default: 168.6969
    gamma2:
      description: Smooth gamma2
      default: 51.1301
    gamma3:
      description: Smooth gamma3
      default: 26.8084

- config: train-predict-flags
  flags:
    df_dim:
      description: DF dimension
      default: 96
    gf_dim:
      description: GF dimension
      default: 48
    z_dim:
      description: X dimension
      default: 100
    r_num:
      description: R
      default: 3

- model: damsm
  description: Deep Attentional Multimodal Similarity Model (DAMSM)

  resources: # Files required by operations
    config:
      description:
        Path to project configuration files.

        The AttnGAN project relies on default configuraiton located in
        the cfg directory. Some config values can be overridden by
        command line arguments to the operation script.
      sources:
        - file: cfg

    data:
      description: Path to data used for training and test.
      sources:
        - file: ../data/photosynthesis

    pretrained-model:
      description:
        Models (pth files) generated by the pretrain operation.

        Selects the latest image and text encoders (determined by
        highest epoch value in file name) and names the files
        'image_encoder.pth' and 'text_encoder.pth' respectively.
      path: nets
      sources:
        - operation: pretrain
          select-max:
            - '.+/Model/image_encoder([0-9]+)\.pth'
            - '.+/Model/text_encoder([0-9]+)\.pth'
          rename:
            pattern: '[0-9]+'
            repl: ''

    trained-model:
      description:
        Models (pth files) generated by the train operation.

        Selects the latest netG model (determined by highest epoch
        value in file name) and names the file 'netG.pth'.
      path: nets
      sources:
        - operation: train
          select-max:
            - '.+/Model/netG_epoch_([0-9]+).pth'
          rename:
            pattern: _epoch_[0-9]
            repl: ''

  operations:
    pretrain:
      description: Pretrain the DAMSM model

      main: pretrain_DAMSM
             --data_dir photosynthesis
             --output_dir output
             --cfg cfg/DAMSM/photosynthesis.yml

      requires:
        - config
        - data

      flags-import: no
      flags:
        $include:
          - common-flags
          - pretrain-train-flags
        delete_captions_pickle:
          description: Force re-create of captions pickle
          arg-switch: yes
          default: yes
        encoder_lr:
          description: Encoder learning rate
          default: 0.0382
        rnn_grad_clip:
          description: RNN gradient clip
          default: 197

        # Uncomment below to override flag defaults for pretrain
        # epochs: 201
        # gamma1: 72.4953
        # gamma2: 65.3585
        # gamma3: 89.2507
        # encoder_lr: 0.003189
        # embedding_dim: 128
        # rnn_grad_clip: 2.0
        # captions_per_image: 1
        # words_num: 244

      output-scalars:
        step: 'end epoch +([0-9]+)'
        s_loss: 'valid loss +([0-9\.]+)'
        w_loss: 'valid loss +[0-9\.]+ +([0-9\.]+)'

      objective: w_loss

    train:
      description: Train the DAMSM model
      main: main
             --cfg cfg/photosynthesis_attn2.yml
             --data_dir photosynthesis
             --net_e nets/text_encoder.pth
             --output_dir output

      requires:
        - config
        - data
        - pretrained-model

      flags-import: no
      flags:
        $include:
          - common-flags
          - pretrain-train-flags
          - train-predict-flags
        discriminator_lr:
          description: Discriminator learning rate
          default: 0.0002
        generator_lr:
          description: Generator learning rate
          default: 0.0002
        lambda:
          description: Lambda
          default: 50

      output-scalars:
        step: 'Step: ([0-9]+)'
        errD0: 'errD0: (\S+)'
        errD1: 'errD1: (\S+)'
        errD2: 'errD2: (\S+)'
        g_loss0: 'g_loss0: (\S+)'
        g_loss1: 'g_loss1: (\S+)'
        g_loss2: 'g_loss2: (\S+)'
        w_loss: 'w_loss: (\S+)'
        s_loss: 's_loss: (\S+)'
        kl_loss: 'kl_loss: (\S+)'

      objective: w_loss

    predict:
      description: Predict on trained model
      main: main
             --predict
             --cfg cfg/eval_photosynthesis.yml
             --data_dir photosynthesis
             --net_e nets/text_encoder.pth
             --net_g nets/netG.pth
             --output_dir output

      requires:
        - config
        - data
        - pretrained-model
        - trained-model

      flags-import: no
      flags:
        $include:
          - common-flags
          - train-predict-flags
        all_captions:
          description: Generate images for all captions
          default: no
          arg-switch: yes
          arg-name: b_validation

    _check:
      steps:
        - run: pretrain epochs=1 --needed
        - run: train epochs=1 --needed
        - run: predict
