{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import io\n",
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key\n",
    "import os\n",
    "from google_images_download import google_images_download\n",
    "import glob\n",
    "import PIL\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "import textacy\n",
    "import en_core_web_sm\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from IPython.display import Image\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training data for text to image model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tblName = \"ResourceDocuments\"\n",
    "nodeIdentifierName = \"photosynthesis-photosynthesis-photosynthesis-biology\"\n",
    "\n",
    "txtToImage_data_dir = 'data/photosynthesis'\n",
    "imageLog_fir='logs'\n",
    "\n",
    "resourceDbName = 'dynamodb'\n",
    "s3Bucket = \"egm-bucket/TEXT_TO_IMAGE_DATA/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from Resource Db: \n",
    "photosynthesis whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NUmber of Items in ResourceDb: 14'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Definitions for photosynthesis from dynamodb\n",
    "\n",
    "## Connect to dynamodb\n",
    "dynamodbClient = boto3.resource(\"dynamodb\")\n",
    "# client = boto3.client('dynamodb')\n",
    "# display(client.describe_table(TableName=tblName))\n",
    "\n",
    "## Connect to table with resources\n",
    "resourceTbl = dynamodbClient.Table(tblName)\n",
    "# display(resourceTbl.global_secondary_indexes)\n",
    "display(\"NUmber of Items in ResourceDb: {}\".format(resourceTbl.item_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text into pandas \n",
    "- For data munging\n",
    "    - stats\n",
    "    - Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Db Response Shape: (14, 13)\n",
      "Db Response Shape: (7, 13)\n",
      "Index(['IMAGES', 'NODE_IDENTIFIER', 'POS', 'RESOURCE', 'RESOURCE_ATTRIBUTION',\n",
      "       'RESOURCE_DATATYPE', 'RESOURCE_SOURCE', 'RESOURCE_TYPE', 'RESOURCE_URL',\n",
      "       'TERM', 'TIME_DOWNLOADED', 'TOPIC', 'UNIQUE_IDENTIFIER'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "response = resourceTbl.query(\n",
    "    IndexName='NODE_IDENTIFIER-index',\n",
    "    KeyConditionExpression=Key('NODE_IDENTIFIER').eq(nodeIdentifierName)\n",
    ")\n",
    "\n",
    "# Pass through pandas for some data munging\n",
    "rsrc_df = pd.DataFrame(response[\"Items\"])\n",
    "print(\"Db Response Shape: {}\".format(rsrc_df.shape))\n",
    "\n",
    "rsrc_df.drop_duplicates(['RESOURCE'], keep='last', inplace=True)\n",
    "rsrc_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Db Response Shape: {}\".format(rsrc_df.shape))\n",
    "print(rsrc_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4        century\n",
       "5        wordnet\n",
       "9     wiktionary\n",
       "10     wikipedia\n",
       "11         gcide\n",
       "12     wikipedia\n",
       "13    ahd-legacy\n",
       "Name: RESOURCE_SOURCE, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsrc_df[\"RESOURCE_SOURCE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Labels for text to image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentenses: 1\n",
      "Number of Sentenses: 1\n",
      "Number of Sentenses: 1\n",
      "Number of Sentenses: 16\n",
      "Number of Sentenses: 9\n",
      "Number of Sentenses: 5\n",
      "Number of Sentenses: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def handle_missing_directories(directory_flpth):\n",
    "    # Handle missing Directory\n",
    "    if not os.path.exists(directory_flpth):\n",
    "        \n",
    "        os.makedirs(directory_flpth)\n",
    "        print(\"Made new directory: {}\".format(directory_flpth))\n",
    "        # print(os.path.join(dirname, flpth))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return\n",
    "\n",
    "# Create text file for each doc - Each Doc maps to an image\n",
    "\n",
    "## TODO: incoroporate number of labals per line\n",
    "def labels_to_imageTxt_files(rsrc_df, trainingData_term, trainigData_flpth='../data'):\n",
    "    \n",
    "    # Handle if a data directory for a term exists e.g. data/photosynthesis\n",
    "    dirname = os.path.abspath('')\n",
    "    termData_flpth = os.path.join(dirname, trainigData_flpth)\n",
    "    handle_missing_directories(termData_flpth)\n",
    "    \n",
    "    \n",
    "    ### Move resource df to textacy\n",
    "   \n",
    "    # Load into textacy to delimit sentences\n",
    "    img_labels = rsrc_df.to_dict(orient=\"records\")\n",
    "    text_stream, metadata_stream = textacy.io.split_records(img_labels, 'RESOURCE')\n",
    "\n",
    "    # Load english model\n",
    "    en = en_core_web_sm.load()\n",
    "    labels_corpus = textacy.Corpus(lang=en, texts=text_stream, metadatas=metadata_stream)\n",
    "    \n",
    "    caption_filename_path = os.path.join(trainigData_flpth, \"captions.pickle\")\n",
    "    \n",
    "    # Loop through corpus and write document to flpth (s3)\n",
    "    ''' Each doc in a corpus equals and image'''\n",
    "    for ix, doc in enumerate(labels_corpus):\n",
    "        print(\"Number of Sentenses: {}\".format(doc.n_sents))\n",
    "        \n",
    "        # Paths to directories (Where to write the text files)\n",
    "        filename = \"{}_{}.txt\".format(trainingData_term, ix)\n",
    "        path_to_file = \"{}/{}\".format(trainigData_flpth, filename)\n",
    "        \n",
    "        # Write captions for google images\n",
    "        f =  open(path_to_file, 'w')\n",
    "        \n",
    "        # Parse Document into sentences\n",
    "        for sent in doc.sents:\n",
    "            caption = textacy.preprocess.preprocess_text(sent.text,\n",
    "                                               lowercase=True,\n",
    "                                               no_punct=True\n",
    "                                              )\n",
    "            # f.write(label+\"\\n\" )\n",
    "            f.write(caption+\" \" )\n",
    "            \n",
    "        f.close()\n",
    "        \n",
    "    return ix + 1 # Count using 1 as start\n",
    "\n",
    "# process labels for images\n",
    "\n",
    "\n",
    "trainingData_term = 'photosynthesis'\n",
    "txt_trainingData_flpth='{}/text'.format(txtToImage_data_dir, 'text')\n",
    "\n",
    "numText_files = labels_to_imageTxt_files(rsrc_df, trainingData_term, txt_trainingData_flpth)\n",
    "numText_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a text file for all of the caption filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data Directory: data/photosynthesis/text\n",
      "\n",
      "data/photosynthesis/text/photosynthesis_4.txt\n",
      "data/photosynthesis/text/photosynthesis_0.txt\n",
      "data/photosynthesis/text/photosynthesis_5.txt\n",
      "data/photosynthesis/text/photosynthesis_6.txt\n",
      "data/photosynthesis/text/photosynthesis_3.txt\n",
      "data/photosynthesis/text/photosynthesis_1.txt\n",
      "data/photosynthesis/text/photosynthesis_2.txt\n",
      "data/photosynthesis/text/sun/sun_01.txt\n"
     ]
    }
   ],
   "source": [
    "# Make a text file with a list of the caption filenames\n",
    "\n",
    "text_flpth = os.path.join(txtToImage_data_dir, 'text')\n",
    "print(\"Text Data Directory: {}\\n\".format(text_flpth))\n",
    "\n",
    "# Name of file with all the caption file names \n",
    "\n",
    "write_filename = \"{}.txt\".format('filenames')\n",
    "filenames_flpth = os.path.join(txtToImage_data_dir, write_filename)\n",
    "\n",
    "\n",
    "# # Make a new text \n",
    "# handle_missing_directories(text_flpth)\n",
    "\n",
    "f =  open(filenames_flpth, 'w')\n",
    "\n",
    "# Find all the files with captions in the text directory and write there names to a file\n",
    "for path, subdirs, files in os.walk(text_flpth):\n",
    "    for name in files:\n",
    "        \n",
    "        caption_flpth = os.path.join(path, name)\n",
    "        \n",
    "        f.write(\"{}\\n\".format(caption_flpth))\n",
    "        print (caption_flpth)\n",
    "    \n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filenames_to_pickle(objTopickle, flpth):\n",
    "    \n",
    "    \n",
    "    \n",
    "    return\n",
    "    \n",
    "def splitData(trainSplit, testSplit, filename_lst):\n",
    "    \n",
    "    # Calculate total number of filenames\n",
    "    num_filenames = len(filename_lst)\n",
    "\n",
    "    numTrain_files = np.ceil(trainSplit * num_filenames).astype(int)\n",
    "    numTest_files = np.floor(testSplit * num_filenames).astype(int)\n",
    "    \n",
    "    print(\"Number of Train files: {}\".format(numTrain_files))\n",
    "    print(\"Number of Test files: {}\".format(numTest_files))\n",
    "    \n",
    "    trainFile_lst = filename_lst[:numTrain_files]\n",
    "    testFile_lst = filename_lst[-numTrain_files:]\n",
    "    \n",
    "    return {\n",
    "        \"train\": trainFile_lst,\n",
    "        \"test\": testFile_lst\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data Directory: data/photosynthesis/text\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/photosynthesis/text/photosynthesis_4.txt',\n",
       " 'data/photosynthesis/text/photosynthesis_0.txt',\n",
       " 'data/photosynthesis/text/photosynthesis_5.txt',\n",
       " 'data/photosynthesis/text/photosynthesis_6.txt',\n",
       " 'data/photosynthesis/text/photosynthesis_3.txt',\n",
       " 'data/photosynthesis/text/photosynthesis_1.txt',\n",
       " 'data/photosynthesis/text/photosynthesis_2.txt',\n",
       " 'data/photosynthesis/text/sun/sun_01.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 8\n",
      "Number of Train files: 6\n",
      "Number of Test files: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': ['data/photosynthesis/text/photosynthesis_4.txt',\n",
       "  'data/photosynthesis/text/photosynthesis_0.txt',\n",
       "  'data/photosynthesis/text/photosynthesis_5.txt',\n",
       "  'data/photosynthesis/text/photosynthesis_6.txt',\n",
       "  'data/photosynthesis/text/photosynthesis_3.txt',\n",
       "  'data/photosynthesis/text/photosynthesis_1.txt'],\n",
       " 'test': ['data/photosynthesis/text/photosynthesis_5.txt',\n",
       "  'data/photosynthesis/text/photosynthesis_6.txt',\n",
       "  'data/photosynthesis/text/photosynthesis_3.txt',\n",
       "  'data/photosynthesis/text/photosynthesis_1.txt',\n",
       "  'data/photosynthesis/text/photosynthesis_2.txt',\n",
       "  'data/photosynthesis/text/sun/sun_01.txt']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Text data directories\n",
    "text_flpth = os.path.join(txtToImage_data_dir, 'text')\n",
    "print(\"Text Data Directory: {}\\n\".format(text_flpth))\n",
    "\n",
    "# Get all the file names\n",
    "captions_filename_lst = glob.glob(text_flpth+\"/**/*.txt\", recursive=True)\n",
    "\n",
    "\n",
    "display(captions_filename_lst)\n",
    "print(\"Number of files: {}\".format(num_filenames))\n",
    "\n",
    "# Split the data into training and test\n",
    "## Will need to accomodate term weightings and try different cossvalidation methods\n",
    "trainSplit = 0.7\n",
    "testSplit = 1\n",
    "\n",
    "split_dict = splitData(trainSplit, testSplit, captions_filename_lst)\n",
    "\n",
    "# write files names to text and pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/photosynthesis/text/photosynthesis_1.txt',\n",
       " 'data/photosynthesis/text/photosynthesis_2.txt',\n",
       " 'data/photosynthesis/text/sun/sun_01.txt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "with open('outfile', 'wb') as fp:\n",
    "    pickle.dump(itemlist, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['photosynthesis_4.txt',\n",
       " 'photosynthesis_0.txt',\n",
       " 'photosynthesis_5.txt',\n",
       " 'photosynthesis_6.txt',\n",
       " 'photosynthesis_3.txt',\n",
       " 'photosynthesis_1.txt',\n",
       " 'photosynthesis_2.txt']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob1(text_flpth,\"*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test and \n",
    "Notes:\n",
    "    - I think in the original AttnGAN code `test` means `cross-validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Images from google "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_images(term ,img_args):\n",
    "    \n",
    "    # Download Images \n",
    "    response = google_images_download.googleimagesdownload()\n",
    "    img_paths = response.download(img_args)\n",
    "    \n",
    "    # Post Process google image results\n",
    "    for idx, f in enumerate(img_paths[term]):\n",
    "        \n",
    "        # Open Google image resulst and conver to jpeg\n",
    "        img = PIL.Image.open(f)\n",
    "        img_filetype = img.format.lower()  # 'JPEG'\n",
    "        \n",
    "        rgb_img = img.convert('RGB')\n",
    "        img.close()\n",
    "        \n",
    "        # Make new filenme to allign with text file name\n",
    "        filename = \"{}_{}.{}\".format(trainingData_term, idx, 'jpg')\n",
    "        newfilepath_f = os.path.join(os.path.dirname(f), filename)\n",
    "        \n",
    "        # Save and image\n",
    "        rgb_img.save(newfilepath_f)\n",
    "        os.remove(f)\n",
    "    \n",
    "    \n",
    "    return response \n",
    "\n",
    "img_args = {\"keywords\":\"photosynthesis\",\n",
    "             \"format\": \"png\",\n",
    "              \"limit\": numText_files,\n",
    "             \"output_directory\": 'data',\n",
    "            \"metadata\": True,\n",
    "            \"image_directory\": \"photosynthesis/images\",\n",
    "            \"no_download\": False,\n",
    "            \"extract_metadata\":True\n",
    "            # \"size\":\"icon\"\n",
    "           }\n",
    "\n",
    "response = download_images(trainingData_term, img_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "347\n"
     ]
    }
   ],
   "source": [
    "im = PIL.Image.open(\"../AttnGAN/data/birds/CUB_200_2011/images/001.Black_footed_Albatross/Black_Footed_Albatross_0002_55.jpg\")\n",
    "width, height = im.size\n",
    "print(width)\n",
    "print(height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captions filepath: data/photosynthesis/captions.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = txtToImage_data_dir \n",
    "\n",
    "filepath = os.path.join(data_dir, 'captions.pickle')\n",
    "        \n",
    "# train_names = self.load_filenames(data_dir, 'train')\n",
    "# logging.debug(\"train_names: {}\".format(train_names))\n",
    "\n",
    "\n",
    "# test_names = self.load_filenames(data_dir, 'test')\n",
    "# logging.debug(\"test_names: {}\".format(test_names))\n",
    "\n",
    "# logging.debug(\"Check if captions.pickle exists\")\n",
    "if not os.path.isfile(filepath):\n",
    "    print(\"captions filepath: {}\".format(filepath))\n",
    "else:\n",
    "    print(\"Cant find captions picke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/photosynthesis/captions.pickle'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import miscc.utils.mkdir_p as mkdir_p\n",
    "from miscc.utils import build_super_images\n",
    "from miscc.losses import sent_loss, words_loss\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "\n",
    "from datasets import TextDataset\n",
    "from datasets import prepare_data\n",
    "\n",
    "from model import RNN_ENCODER, CNN_ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = (os.path.abspath(os.path.join(os.path.realpath(__file__), './.')))\n",
    "sys.path.append(dir_path)\n",
    "\n",
    "\n",
    "UPDATE_INTERVAL = 200\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train a DAMSM network')\n",
    "    parser.add_argument('--cfg', dest='cfg_file',\n",
    "                        help='optional config file',\n",
    "                        default='cfg/DAMSM/bird.yml', type=str)\n",
    "    parser.add_argument('--gpu', dest='gpu_id', type=int, default=0)\n",
    "    parser.add_argument('--data_dir', dest='data_dir', type=str, default='')\n",
    "    parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def train(dataloader, cnn_model, rnn_model, batch_size,\n",
    "          labels, optimizer, epoch, ixtoword, image_dir):\n",
    "    cnn_model.train()\n",
    "    rnn_model.train()\n",
    "    s_total_loss0 = 0\n",
    "    s_total_loss1 = 0\n",
    "    w_total_loss0 = 0\n",
    "    w_total_loss1 = 0\n",
    "    count = (epoch + 1) * len(dataloader)\n",
    "    start_time = time.time()\n",
    "    for step, data in enumerate(dataloader, 0):\n",
    "        # print('step', step)\n",
    "        rnn_model.zero_grad()\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        imgs, captions, cap_lens, \\\n",
    "            class_ids, keys = prepare_data(data)\n",
    "\n",
    "\n",
    "        # words_features: batch_size x nef x 17 x 17\n",
    "        # sent_code: batch_size x nef\n",
    "        words_features, sent_code = cnn_model(imgs[-1])\n",
    "        # --> batch_size x nef x 17*17\n",
    "        nef, att_sze = words_features.size(1), words_features.size(2)\n",
    "        # words_features = words_features.view(batch_size, nef, -1)\n",
    "\n",
    "        hidden = rnn_model.init_hidden(batch_size)\n",
    "        # words_emb: batch_size x nef x seq_len\n",
    "        # sent_emb: batch_size x nef\n",
    "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
    "\n",
    "        w_loss0, w_loss1, attn_maps = words_loss(words_features, words_emb, labels,\n",
    "                                                 cap_lens, class_ids, batch_size)\n",
    "        w_total_loss0 += w_loss0.data\n",
    "        w_total_loss1 += w_loss1.data\n",
    "        loss = w_loss0 + w_loss1\n",
    "\n",
    "        s_loss0, s_loss1 = \\\n",
    "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
    "        loss += s_loss0 + s_loss1\n",
    "        s_total_loss0 += s_loss0.data\n",
    "        s_total_loss1 += s_loss1.data\n",
    "        #\n",
    "        loss.backward()\n",
    "        #\n",
    "        # `clip_grad_norm` helps prevent\n",
    "        # the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm(rnn_model.parameters(),\n",
    "                                      cfg.TRAIN.RNN_GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % UPDATE_INTERVAL == 0:\n",
    "            count = epoch * len(dataloader) + step\n",
    "\n",
    "            s_cur_loss0 = s_total_loss0[0] / UPDATE_INTERVAL\n",
    "            s_cur_loss1 = s_total_loss1[0] / UPDATE_INTERVAL\n",
    "\n",
    "            w_cur_loss0 = w_total_loss0[0] / UPDATE_INTERVAL\n",
    "            w_cur_loss1 = w_total_loss1[0] / UPDATE_INTERVAL\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                  's_loss {:5.2f} {:5.2f} | '\n",
    "                  'w_loss {:5.2f} {:5.2f}'\n",
    "                  .format(epoch, step, len(dataloader),\n",
    "                          elapsed * 1000. / UPDATE_INTERVAL,\n",
    "                          s_cur_loss0, s_cur_loss1,\n",
    "                          w_cur_loss0, w_cur_loss1))\n",
    "            s_total_loss0 = 0\n",
    "            s_total_loss1 = 0\n",
    "            w_total_loss0 = 0\n",
    "            w_total_loss1 = 0\n",
    "            start_time = time.time()\n",
    "            # attention Maps\n",
    "            img_set, _ = \\\n",
    "                build_super_images(imgs[-1].cpu(), captions,\n",
    "                                   ixtoword, attn_maps, att_sze)\n",
    "            if img_set is not None:\n",
    "                im = Image.fromarray(img_set)\n",
    "                fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n",
    "                im.save(fullpath)\n",
    "    return count\n",
    "\n",
    "\n",
    "def evaluate(dataloader, cnn_model, rnn_model, batch_size):\n",
    "    cnn_model.eval()\n",
    "    rnn_model.eval()\n",
    "    s_total_loss = 0\n",
    "    w_total_loss = 0\n",
    "    for step, data in enumerate(dataloader, 0):\n",
    "        real_imgs, captions, cap_lens, \\\n",
    "                class_ids, keys = prepare_data(data)\n",
    "\n",
    "        words_features, sent_code = cnn_model(real_imgs[-1])\n",
    "        # nef = words_features.size(1)\n",
    "        # words_features = words_features.view(batch_size, nef, -1)\n",
    "\n",
    "        hidden = rnn_model.init_hidden(batch_size)\n",
    "        words_emb, sent_emb = rnn_model(captions, cap_lens, hidden)\n",
    "\n",
    "        w_loss0, w_loss1, attn = words_loss(words_features, words_emb, labels,\n",
    "                                            cap_lens, class_ids, batch_size)\n",
    "        w_total_loss += (w_loss0 + w_loss1).data\n",
    "\n",
    "        s_loss0, s_loss1 = \\\n",
    "            sent_loss(sent_code, sent_emb, labels, class_ids, batch_size)\n",
    "        s_total_loss += (s_loss0 + s_loss1).data\n",
    "\n",
    "        if step == 50:\n",
    "            break\n",
    "\n",
    "    s_cur_loss = s_total_loss[0] / step\n",
    "    w_cur_loss = w_total_loss[0] / step\n",
    "\n",
    "    return s_cur_loss, w_cur_loss\n",
    "\n",
    "\n",
    "def build_models():\n",
    "    # build model ############################################################\n",
    "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
    "    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
    "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
    "    start_epoch = 0\n",
    "    if cfg.TRAIN.NET_E != '':\n",
    "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
    "        text_encoder.load_state_dict(state_dict)\n",
    "        print('Load ', cfg.TRAIN.NET_E)\n",
    "        #\n",
    "        name = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
    "        state_dict = torch.load(name)\n",
    "        image_encoder.load_state_dict(state_dict)\n",
    "        print('Load ', name)\n",
    "\n",
    "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
    "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
    "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
    "        start_epoch = int(start_epoch) + 1\n",
    "        print('start_epoch', start_epoch)\n",
    "    if cfg.CUDA:\n",
    "        text_encoder = text_encoder.cuda()\n",
    "        image_encoder = image_encoder.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    return text_encoder, image_encoder, labels, start_epoch\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    if args.cfg_file is not None:\n",
    "        cfg_from_file(args.cfg_file)\n",
    "\n",
    "    if args.gpu_id == -1:\n",
    "        cfg.CUDA = False\n",
    "    else:\n",
    "        cfg.GPU_ID = args.gpu_id\n",
    "\n",
    "    if args.data_dir != '':\n",
    "        cfg.DATA_DIR = args.data_dir\n",
    "    print('Using config:')\n",
    "    pprint.pprint(cfg)\n",
    "\n",
    "    if not cfg.TRAIN.FLAG:\n",
    "        args.manualSeed = 100\n",
    "    elif args.manualSeed is None:\n",
    "        args.manualSeed = random.randint(1, 10000)\n",
    "    random.seed(args.manualSeed)\n",
    "    np.random.seed(args.manualSeed)\n",
    "    torch.manual_seed(args.manualSeed)\n",
    "    if cfg.CUDA:\n",
    "        torch.cuda.manual_seed_all(args.manualSeed)\n",
    "\n",
    "    ##########################################################################\n",
    "    now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    output_dir = '../output/%s_%s_%s' % \\\n",
    "        (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
    "\n",
    "    model_dir = os.path.join(output_dir, 'Model')\n",
    "    image_dir = os.path.join(output_dir, 'Image')\n",
    "    mkdir_p(model_dir)\n",
    "    mkdir_p(image_dir)\n",
    "\n",
    "    torch.cuda.set_device(cfg.GPU_ID)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Get data loader ##################################################\n",
    "    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n",
    "    batch_size = cfg.TRAIN.BATCH_SIZE\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize(int(imsize * 76 / 64)),\n",
    "        transforms.RandomCrop(imsize),\n",
    "        transforms.RandomHorizontalFlip()])\n",
    "    dataset = TextDataset(cfg.DATA_DIR, 'train',\n",
    "                          base_size=cfg.TREE.BASE_SIZE,\n",
    "                          transform=image_transform)\n",
    "\n",
    "    print(dataset.n_words, dataset.embeddings_num)\n",
    "    assert dataset\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, drop_last=True,\n",
    "        shuffle=True, num_workers=int(cfg.WORKERS))\n",
    "\n",
    "    # # validation data #\n",
    "    dataset_val = TextDataset(cfg.DATA_DIR, 'test',\n",
    "                              base_size=cfg.TREE.BASE_SIZE,\n",
    "                              transform=image_transform)\n",
    "    dataloader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=batch_size, drop_last=True,\n",
    "        shuffle=True, num_workers=int(cfg.WORKERS))\n",
    "\n",
    "    # Train ##############################################################\n",
    "    text_encoder, image_encoder, labels, start_epoch = build_models()\n",
    "    para = list(text_encoder.parameters())\n",
    "    for v in image_encoder.parameters():\n",
    "        if v.requires_grad:\n",
    "            para.append(v)\n",
    "    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n",
    "    # At any point you can hit Ctrl + C to break out of training early.\n",
    "    try:\n",
    "        lr = cfg.TRAIN.ENCODER_LR\n",
    "        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n",
    "            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n",
    "            epoch_start_time = time.time()\n",
    "            count = train(dataloader, image_encoder, text_encoder,\n",
    "                          batch_size, labels, optimizer, epoch,\n",
    "                          dataset.ixtoword, image_dir)\n",
    "            print('-' * 89)\n",
    "            if len(dataloader_val) > 0:\n",
    "                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n",
    "                                          text_encoder, batch_size)\n",
    "                print('| end epoch {:3d} | valid loss '\n",
    "                      '{:5.2f} {:5.2f} | lr {:.5f}|'\n",
    "                      .format(epoch, s_loss, w_loss, lr))\n",
    "            print('-' * 89)\n",
    "            if lr > cfg.TRAIN.ENCODER_LR/10.:\n",
    "                lr *= 0.98\n",
    "\n",
    "            if (epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0 or\n",
    "                epoch == cfg.TRAIN.MAX_EPOCH):\n",
    "                torch.save(image_encoder.state_dict(),\n",
    "                           '%s/image_encoder%d.pth' % (model_dir, epoch))\n",
    "                torch.save(text_encoder.state_dict(),\n",
    "                           '%s/text_encoder%d.pth' % (model_dir, epoch))\n",
    "                print('Save G/Ds models.')\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 89)\n",
    "        print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.dirname(\"/home/ec2-user/environment/AttnGAN/data/photosynthesis/images/5. photosynthesis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "s3Client = boto3.client(\"s3\")     \n",
    "s3Client.Object('my-bucket-name', 'newfile.txt').put(Body=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(labels_corpus.docs[0].sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(\"https://ssec.si.edu/stemvisions-blog/what-photosynthesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-ml",
   "language": "python",
   "name": "py36-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
